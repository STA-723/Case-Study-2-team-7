---
title: "R Notebook"
output: html_notebook
---
This file runs the model to deal with spacial autocorrelation. 
```{r}
library(tidyverse)
library(ggpubr)
require(rstan)
library(rray)
library(caramellar)
```

Import the data and set some variables
```{r}
# Import the data
data = read.csv("AB_NYC_2019.csv")
# Include the variable that detects if the renting host is a couple or a single 
# person/company
data =data %>%
  mutate(couple_renting = (stringr::str_detect(host_name, '&| And | Y |\\+| and ')*1 )) 
# If number of reviews is missing, set it to 0
data[is.na(data$reviews_per_month),]$reviews_per_month = 0
# Drop rows where price==0
data = data[data$price>0,]
# Take the log of prices
```

Our model is the following 
$$
\begin{align}
y_i = \alpha + r_i + &B_{k_i} + \mathbf{x}_i^T\boldsymbol{\beta} + \eta_{l_i} + \varepsilon_i \\
&\varepsilon_i \stackrel{iid}{\sim} \mathcal{N}(0,1/\phi) \\
&p(\alpha, \phi) \propto 1/\phi\\
&\boldsymbol{\beta} \sim \mathcal{N}(\mathbf{0},g(\mathbf{X}^T\mathbf{X})^{-1}/\phi)\\
&g\sim InvGamma(1/2,1/2) \\
&r_i \stackrel{iid}{\sim} \mathcal{N}(0, 1/\phi)\\
&B_{k_i} {\sim} \mathcal{N}(\beta_{k_i}, g_B/\phi)\\
&\beta_{k_i} \stackrel{iid}{\sim} \mathcal{N}(0, 1)\\
&g_B \sim InvGamma(1/2,1/2) \\
&\eta_{l_i} = \mathcal{N}(0, \mathbf{K}^{-1})\\
& \mathbf{K} \sim Wish_G(3, \mathbf{D}^{-1})\\
&\mathbf{D} = E_W - \rho W\\
&\rho \sim U(0,1)
\end{align}
$$
where $W$ is the adjacency matrix. We construct it in three ways.
1) Use the euclidean distance across all the neighborhoods
```{r}
compute_distance = function(coord){
  return(sqrt(coord[1]^2+coord[2]^2))
}
neighbr_names = sort(unique(data$neighbourhood))
# Step 1 - compute mean latitude and longitude
df_ng = data %>%
  group_by(neighbourhood) %>%
  dplyr::summarize_at(c("longitude", "latitude"), mean, na.rm=TRUE)
# Step 2 - initialize the adjacency matrix
adjMat = matrix(0, nrow=length(neighbr_names),ncol=length(neighbr_names))
colnames(adjMat) = rownames(adjMat) = neighbr_names
# Step 3 - Insert the respective measures of distance
for(n in neighbr_names){
  mat_coord = matrix(0, nrow=nrow(df_ng), ncol=2)
  # Sum the coordiantes acroos neighboorhoods
  neigh = df_ng[df_ng$neighbourhood==n, ][,-1]
  mat_coord[,1] = as.matrix(df_ng[, -1])[,1] - as.numeric(neigh[1])
  mat_coord[,2] = as.matrix(df_ng[, -1])[,2] - as.numeric(neigh[2])
  # Compute the euclidean distance
  vec_coord = apply(mat_coord, 1, compute_distance)
  adjMat[which(neighbr_names==n),] = vec_coord
}
# Finally, compute W by setting a treshold
W = ifelse(adjMat <0.032,1,0)
```
```{r}
E_W = diag(apply(W, 2, sum))
# test
solve(E_W[-1,-1] - W[-1,-1])
solve(W)

round(eigen(W)$values, 4)


W[113,]


which(duplicated(W) | duplicated(W[nrow(W):1, ])[nrow(W):1])
W[,c(which(duplicated(W) | duplicated(W[nrow(W):1, ])[nrow(W):1]))]
```

2) Use min-max argument and check with euclidean distance.
```{r}
neighbr_names = sort(unique(data$neighbourhood))
# Step 1 - compute mean latitude and longitude
df_ng_max = data %>%
  group_by(neighbourhood) %>%
  dplyr::summarize_at(c("longitude", "latitude"), max, na.rm=TRUE)
df_ng_min = data %>%
  group_by(neighbourhood) %>%
  dplyr::summarize_at(c("longitude", "latitude"), min, na.rm=TRUE)
df_ng = left_join(df_ng_max, df_ng_min, by=c("neighbourhood"))
# Step 2 - initialize the adjacency matrix
adjMat = diag(length(neighbr_names))
colnames(adjMat) = rownames(adjMat) = neighbr_names
# Step 3 - Insert the respective measures of distance
for(n in neighbr_names){
  # Subtract max/min latitude and longitude from each point
  neig_index = which(df_ng$neighbourhood==n)
  neigh_allpoints = data[data$neighbourhood==n, ]
  neigh = df_ng[df_ng$neighbourhood==n,]
  neigh[,c(1,2,3,4,5)] = neigh[,c(1,4,5,2,3)] 
  reduced_distances = (abs(as.matrix(df_ng[-1])%b-% matrix(unlist(c(neigh)[-1]), 1))<0.005)*1
  # Check if the adjacency exists
  for(d in 1:4){
    candidates = which(reduced_distances[,d]==1)
    for(candidate in candidates){
      candidate_ng = data[data$neighbourhood==neighbr_names[candidate], ]
      if(d==1){
        # Min Long neigh similar to Max Long candidate ng candidate
        coord_ng = as.vector(neigh_allpoints[neigh_allpoints$longitude == min(neigh_allpoints$longitude),c(7,8)][1,])
        coord_ng_candidate = as.vector(candidate_ng[candidate_ng$longitude == max(candidate_ng$longitude),c(7,8)][1,])
      } else if(d==2){
          # Min Lat neigh similar to Max Lat candidate ng candidate
        coord_ng = as.vector(neigh_allpoints[neigh_allpoints$latitude == min(neigh_allpoints$latitude),c(7,8)][1,])
        coord_ng_candidate = as.vector(candidate_ng[candidate_ng$latitude == max(candidate_ng$latitude),c(7,8)][1,])
      } else if(d==3){
        # Max Long neigh similar to Min Long candidate ng candidate
        coord_ng = as.vector(neigh_allpoints[neigh_allpoints$longitude == max(neigh_allpoints$longitude),c(7,8)][1,])
        coord_ng_candidate = as.vector(candidate_ng[candidate_ng$longitude == min(candidate_ng$longitude),c(7,8)][1,])
      } else {
        # Max Lat neigh similar to Min Lat candidate ng candidate
        coord_ng = as.vector(neigh_allpoints[neigh_allpoints$latitude == max(neigh_allpoints$latitude),c(7,8)][1,])
        coord_ng_candidate = as.vector(candidate_ng[candidate_ng$latitude == min(candidate_ng$latitude),c(7,8)][1,])
      }
        # Compute euclidean distance between the selected points
        dist = as.integer((sqrt((coord_ng[1]-coord_ng_candidate[1])^2 + (coord_ng[2]-coord_ng_candidate[2])^2)<0.02)*1)
        # Include the value in the matrix
        adjMat[neig_index, candidate] = dist
    }
  }  
}
W=adjMat 
```
And check that W is all right
```{r}
# Check if symmetric
for(n in 1:length(neighbr_names)){
  row = W[n,]
  col = W[,n]
  if(sum(row==col)!=221){
    diff = which(row!=col)
    for(d in diff){
      W[n,d] = W[d,n] = 0
    }
  }
}
# Check if repeated columns
which(duplicated(W) | duplicated(W[nrow(W):1, ])[nrow(W):1])
dup = W[,c(which(duplicated(W) | duplicated(W[nrow(W):1, ])[nrow(W):1]))]
# Fix repeated columns (by hand...)
W[2,102] = W[102,2] = 0 # Huguenot and Arden Heights do not strictly match
W[68,4] = W[4,68] = 1 # Edgemere and Arverne are connected
W[90,121 ] = W[121 ,90] = 0 # manhattan beach is not close to Gravesand
W[213,138 ] = W[138,213] = 1 # whitestone is connected to murray hill
W[213,9 ] = W[9,213] = 1 #whitestone is connected to bay terrace
W[212,123 ] = W[123,212] = 1 #Marines harbor is connected to westerleigh
# Compute E_W
E_W = diag(apply(W, 2, sum))
# test Inveribility
u = runif(1)
s = solve(E_W - u*W)
```

3) Use voronoi tasellation
```{r}
# Sample (with replacement) 1000 points for each neighbourhood
adjMat_final  = matrix(0, nrow=221, ncol=221)
nSamples =1000
for(trial in 1:nSamples){
  # Sample one point in each neighborhoods
  coord_neighs = data %>% 
    dplyr::select(neighbourhood, latitude, longitude) %>% 
    group_by(neighbourhood) %>% sample_n(1) 
  # compute the voronoi adjacency
  W_voronoi = voronoi_adjacency(coord_neighs, formula = neighbourhood ~ latitude + longitude)
  adjMat_voronoi = (W_voronoi$Adjacencies)*1
  diag(adjMat_voronoi) = rep(1, 221)
  colnames(adjMat_voronoi) = rownames(adjMat_voronoi) = neighbr_names
  # Sum it with the previous ones
  adjMat_final = adjMat_final + adjMat_voronoi
}
adjMat_voronoi = (adjMat_final/nSamples > 0.8)*1
```
check the differece between voronoi and my method
```{r}
check1 = (adjMat_voronoi > W)*1
subset(check1, check1[2,] == 1)
```
Voronoi seems more efficient. It detects things that my method does not. Good to know, we are going to use voronoi for now.
```{r}
# Compute the matrixes needed for the G wishart specification
W = adjMat_voronoi
E_W = diag(apply(W, 1, sum))
# Check if it is invertible under paper formulation
rho = runif(1)
solve(E_W-rho*W)

```

## Import the cleared data from Xiaojun
```{r}
df_final = AB_NYC_2019_final
quantile(df_final$price)
```
Build now the input quantities for the STAN file
```{r}
# We assign a g-prior over the continuous variables
# reviews_per_month,calculated_host_listings_count, minimum_nights, months of total activitys
df_final = df_final %>% mutate(avail_cat = case_when(availability_365==0 ~"Low", 
                                      availability_365==1 ~"Medium", 
                                      availability_365==2 ~"High"))
df_final$months_of_activity = round(df_final$number_of_reviews/df_final$reviews_per_month)
# X = matrix of continuous variables
X = model.matrix(price ~-1 + reviews_per_month + calculated_host_listings_count +
                  minimum_nights + months_of_activity, data=df_final)
invXtX = solve(t(X)%*%X) # covariance matrix in the Zellen's g prior
```
Add now the dummy columns
```{r}
# Dummies for the Boroughs effects
B = model.matrix(price ~-1 + neighbourhood_group, data=df_final)[,-1]
# Dummies for the Neighbourhood effect
NB = model.matrix(price ~-1 + neighbourhood, data=df_final)
# Dummies for the room type
RType = model.matrix(price ~-1 + room_type, data=df_final)[,-3]
# Dummies for type of availability
Av365 = model.matrix(price ~-1 + avail_cat, data=df_final)[,-1]
# Dummies for individual vs couple renting
df_final =df_final %>%
  mutate(couple_renting = (stringr::str_detect(host_name, '&| And | Y |\\+| and ')*1))
Couple = df_final$couple_renting
```
Finally, the words columns
```{r}
# Need to ask Xiaojun
```
And now, initialize the stan model, and run it
```{r}
N = dim(X)[1]
y = log(df_final$price)
# continuous vars
p_cont = dim(X)[2]
beta0 = rep(0, p_cont)
# discrete vars
p_borough = ncol(B)
p_room = ncol(RType)
p_avl = ncol(Av365)
# Neighborhood
p_neigh = ncol(NB)
eta0 = rep(0, p_neigh)
# Stan List
stan_dat = list(N = N, 
                y = y,
                p_cont = p_cont,
                X=X, 
                invXtX=invXtX,
                beta0=beta0,
                p_borough = p_borough,
                B=B,
                p_room = p_room, 
                RType= RType,
                p_avl=p_avl,
                Avl=Av365,
                Couple=Couple,
                p_neigh=p_neigh,
                NB=NB,
                eta0=eta0,
                W=W,
                E_W=E_W)
```
 
```{r}
# And now, run the model...
fit = stan('gWishart_model.stan', data = stan_dat, chains = 2, refresh = 0, verbose=TRUE)
output = rstan::extract(fit)
```


```{r}
stan_dat_reduced = list(N = N, 
                y = y,
                p_neigh=p_neigh,
                NB=NB,
                eta0=eta0,
                W=W)
# Reduced model
fit_red = stan('gWishart_model_reduced.stan', data = stan_dat, chains = 2, refresh = 0, verbose=TRUE)
output = rstan::extract(fit_red)
```














